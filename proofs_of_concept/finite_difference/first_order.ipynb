{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1087fbb10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "if torch.backends.mps.is_available(): # Apple Silicon\n",
    "    device = torch.device(\"mps\")\n",
    "if torch.cuda.is_available(): # Nvidia GPU\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def periodic_differences(x):\n",
    "    forward_diff = torch.roll(x, -1, dims=1) - x\n",
    "    backward_diff = x - torch.roll(x, 1, dims=1)\n",
    "    central_diff = 0.5*(torch.roll(x, -1, dims=1) - torch.roll(x, 1, dims=1))\n",
    "\n",
    "    return forward_diff, central_diff, backward_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FiniteDifferenceNet(nn.Module):\n",
    "    \"\"\"Neural network to learn first-order finite differences\n",
    "\n",
    "    Demonstrate that a 1D convolutional layer can be used to estimate\n",
    "    first-order finite differences [forward, central, and backward].\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(FiniteDifferenceNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=3, kernel_size=3, padding=0, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.2484e-09, -1.0000e+00,  1.0000e+00]],\n",
      "\n",
      "        [[-5.0000e-01,  5.4967e-10,  5.0000e-01]],\n",
      "\n",
      "        [[-1.0000e+00,  1.0000e+00, -1.1383e-10]]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(10,50)\n",
    "a[:,0] = a[:,-1]\n",
    "gradx = periodic_differences(a)\n",
    "fd_true = torch.stack(gradx, dim=1)[..., 1:-1]\n",
    "fd_true = fd_true.to(device)\n",
    "\n",
    "fd_net = FiniteDifferenceNet().to(device)\n",
    "fd_loss = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(fd_net.parameters(), lr=1e-1)\n",
    "\n",
    "a_channel = a.unsqueeze(dim=1).to(device)\n",
    "\n",
    "fd_net.train()\n",
    "for i in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    fd_pred = fd_net(a_channel)\n",
    "    loss = fd_loss(fd_pred, fd_true)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (i-1) % 1000 == 0:\n",
    "        print(loss.item(), end=\"\\r\")\n",
    "print(fd_net.conv1.weight.data)\n",
    "# Output for the above should be very close to:\n",
    "#  [  0   -1  1   ]\n",
    "#  [  0.5  0  0.5 ]\n",
    "#  [ -1    1  0   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upwind_difference(x):\n",
    "    forward, _, backward = periodic_differences(x)\n",
    "\n",
    "    sign = torch.sign(x)\n",
    "    upwind = forward*(sign < 0) + backward*(sign >= 0)\n",
    "    return upwind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpwindDifferenceSignFunctionNet(nn.Module):\n",
    "    \"\"\"Convolutional neural network with sign function\n",
    "\n",
    "    Trains a convolutional layer that should learn the weights for a forward and\n",
    "    backward finite difference as two channels. A sign function (similar to an\n",
    "    actual upwind difference scheme) is used to \"choose\" which channel is output\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(UpwindDifferenceNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=2, kernel_size=3, padding=0, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        sign = torch.sign(x)[:,:,1:-1].squeeze()\n",
    "        x = self.conv1(x)\n",
    "        x = x[:,0,:]*(sign < 0) + x[:,1,:]*(sign >= 0)\n",
    "        x = x.unsqueeze(1)\n",
    "        return x\n",
    "\n",
    "class UpwindDifferenceNet(nn.Module):\n",
    "    \"\"\"Convolutional neural network with learned weights\n",
    "\n",
    "    Trains a convolutional layer that should learn the weights for a forward and\n",
    "    backward finite difference as two channels. Two linear layers with sigmoid\n",
    "    activation functions predict how the two channels of the convolutional layer\n",
    "    should be combined for the final output. An extra factor is included to make\n",
    "    the sigmoid activation function behave more like a heaviside function while\n",
    "    still being differentiable with non-zero derivatives.\n",
    "\n",
    "    Note: The two weights could be combined into one linear layer with two\n",
    "    outputs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(UpwindDifferenceNet, self).__init__()\n",
    "\n",
    "        self.steepen_sigmoid_factor = 10 # Make the sigmoid more like a heaviside function\n",
    "        self.channel_wt1 = nn.Linear(1,1)\n",
    "        self.channel_wt2 = nn.Linear(1,1)\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=2, kernel_size=3, padding=0, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        nbatch, nchannels, nelements = x.shape\n",
    "\n",
    "        channel1_weight = torch.nn.Sigmoid()(self.steepen_sigmoid_factor*self.channel_wt1(\n",
    "            x.reshape(-1,1)).reshape(nbatch, nelements)[:, 1:-1]\n",
    "        )\n",
    "        channel2_weight = torch.nn.Sigmoid()(self.steepen_sigmoid_factor*self.channel_wt2(\n",
    "            x.reshape(-1,1)).reshape(nbatch, nelements)[:, 1:-1]\n",
    "        )\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = x[:,0,:]*channel1_weight + x[:,1,:]*channel2_weight\n",
    "        x = x.unsqueeze(1)\n",
    "        return x\n",
    "\n",
    "net = UpwindDifferenceNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(10,50)\n",
    "a[:,0] = a[:,-1]\n",
    "uw_true = upwind_difference(a).unsqueeze(1)[..., 1:-1]\n",
    "uw_true = uw_true.to(device)\n",
    "\n",
    "uw_net = UpwindDifferenceNet().to(device)\n",
    "uw_loss = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(uw_net.parameters(), lr=1e-2)\n",
    "\n",
    "a_channel = a.unsqueeze(dim=1).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 3.9986e-05, -1.0002e+00,  1.0002e+00]],\n",
      "\n",
      "        [[-9.9997e-01,  2.0000e+00, -9.9999e-01]]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "for i in range(5000):\n",
    "    uw_net.train()\n",
    "    optimizer.zero_grad()\n",
    "    uw_pred = uw_net(a_channel)\n",
    "    loss = uw_loss(uw_pred, uw_true)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (i-1) % 100 == 0:\n",
    "        print(loss.item(), end=\"\\r\")\n",
    "\n",
    "print(uw_net.conv1.weight.data)\n",
    "# Output should be close to\n",
    "# [ 0  -1  1 ]\n",
    "# [ -1  1  0 ]\n",
    "# The rows could be swapped since there is not restriction around the order of\n",
    "# the channels\n",
    "\n",
    "# Note: One interesting behavior. There seems to be three minima for loss\n",
    "# function: one where the channels correspond to the forward and backward finite\n",
    "# differences, the others being it may either be the forward or backward\n",
    "# derivative for one channel and the second derivative (or its negative) for the\n",
    "# second channel. This likely arises because the \"missing\" finite difference\n",
    "# is just a linear combination of hte other two.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mom6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
